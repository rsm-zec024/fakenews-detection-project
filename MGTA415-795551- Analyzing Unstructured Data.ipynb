{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                           news_url  \\\n",
      "0  gossipcop-882573  https://www.brides.com/story/teen-mom-jenelle-...   \n",
      "1  gossipcop-875924  https://www.dailymail.co.uk/tvshowbiz/article-...   \n",
      "2  gossipcop-894416        https://en.wikipedia.org/wiki/Quinn_Perkins   \n",
      "3  gossipcop-857248  https://www.refinery29.com/en-us/2018/03/19192...   \n",
      "4  gossipcop-884684  https://www.cnn.com/2017/10/04/entertainment/c...   \n",
      "\n",
      "                                               title  \\\n",
      "0  Teen Mom Star Jenelle Evans' Wedding Dress Is ...   \n",
      "1  Kylie Jenner refusing to discuss Tyga on Life ...   \n",
      "2                                      Quinn Perkins   \n",
      "3  I Tried Kim Kardashian's Butt Workout & Am For...   \n",
      "4  Celine Dion donates concert proceeds to Vegas ...   \n",
      "\n",
      "                                           tweet_ids  \n",
      "0  912371411146149888\\t912371528343408641\\t912372...  \n",
      "1  901989917546426369\\t901989992074969089\\t901990...  \n",
      "2  931263637246881792\\t931265332022579201\\t931265...  \n",
      "3  868114761723936769\\t868122567910936576\\t868128...  \n",
      "4  915528047004209152\\t915529285171122176\\t915530...  \n",
      "\n",
      "Columns: Index(['id', 'news_url', 'title', 'tweet_ids'], dtype='object')\n",
      "\n",
      "Missing Values:\n",
      "id              0\n",
      "news_url       13\n",
      "title           0\n",
      "tweet_ids    1058\n",
      "dtype: int64\n",
      "\n",
      "Total Entries: 16817\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取本地 CSV 文件\n",
    "file_path = \"/Users/jerry/Desktop/gossipcop_real.csv\"\n",
    "real_news = pd.read_csv(file_path)\n",
    "\n",
    "# 查看前几行\n",
    "print(real_news.head())\n",
    "\n",
    "# 查看列名\n",
    "print(\"\\nColumns:\", real_news.columns)\n",
    "\n",
    "# 检查是否有缺失值\n",
    "print(\"\\nMissing Values:\")\n",
    "print(real_news.isnull().sum())\n",
    "\n",
    "# 查看数据总数\n",
    "print(\"\\nTotal Entries:\", len(real_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake News Sample:\n",
      "                     id                                           news_url  \\\n",
      "0  gossipcop-2493749932  www.dailymail.co.uk/tvshowbiz/article-5874213/...   \n",
      "1  gossipcop-4580247171  hollywoodlife.com/2018/05/05/paris-jackson-car...   \n",
      "2   gossipcop-941805037  variety.com/2017/biz/news/tax-march-donald-tru...   \n",
      "3  gossipcop-2547891536  www.dailymail.co.uk/femail/article-3499192/Do-...   \n",
      "4  gossipcop-5476631226  variety.com/2018/film/news/list-2018-oscar-nom...   \n",
      "\n",
      "                                               title  \\\n",
      "0  Did Miley Cyrus and Liam Hemsworth secretly ge...   \n",
      "1  Paris Jackson & Cara Delevingne Enjoy Night Ou...   \n",
      "2  Celebrities Join Tax March in Protest of Donal...   \n",
      "3  Cindy Crawford's daughter Kaia Gerber wears a ...   \n",
      "4      Full List of 2018 Oscar Nominations – Variety   \n",
      "\n",
      "                                           tweet_ids  \n",
      "0  284329075902926848\\t284332744559968256\\t284335...  \n",
      "1  992895508267130880\\t992897935418503169\\t992899...  \n",
      "2  853359353532829696\\t853359576543920128\\t853359...  \n",
      "3  988821905196158981\\t988824206556172288\\t988825...  \n",
      "4  955792793632432131\\t955795063925301249\\t955798...  \n",
      "\n",
      "Columns: Index(['id', 'news_url', 'title', 'tweet_ids'], dtype='object')\n",
      "\n",
      "Missing Values:\n",
      "id             0\n",
      "news_url     256\n",
      "title          0\n",
      "tweet_ids    188\n",
      "dtype: int64\n",
      "\n",
      "Total Entries: 5323\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取本地 CSV 文件\n",
    "file_path = \"/Users/jerry/Desktop/gossipcop_fake.csv\"\n",
    "fake_news = pd.read_csv(file_path)\n",
    "\n",
    "# 查看前几行\n",
    "print(\"Fake News Sample:\")\n",
    "print(fake_news.head())\n",
    "\n",
    "# 查看列名\n",
    "print(\"\\nColumns:\", fake_news.columns)\n",
    "\n",
    "# 检查是否有缺失值\n",
    "print(\"\\nMissing Values:\")\n",
    "print(fake_news.isnull().sum())\n",
    "\n",
    "# 查看数据总数\n",
    "print(\"\\nTotal Entries:\", len(fake_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别分布：\n",
      "label\n",
      "1    16817\n",
      "0     5323\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取数据（请确保路径正确）\n",
    "real_news = pd.read_csv(\"/Users/jerry/Desktop/gossipcop_real.csv\")\n",
    "fake_news = pd.read_csv(\"/Users/jerry/Desktop/gossipcop_fake.csv\")\n",
    "\n",
    "# 添加标签：真实新闻为1，假新闻为0\n",
    "real_news[\"label\"] = 1\n",
    "fake_news[\"label\"] = 0\n",
    "\n",
    "# 合并数据集\n",
    "df = pd.concat([real_news, fake_news], ignore_index=True)\n",
    "\n",
    "# 检查类别分布\n",
    "print(\"类别分布：\")\n",
    "print(df[\"label\"].value_counts())\n",
    "\n",
    "# 只保留新闻标题和标签（后续只用标题进行文本分类）\n",
    "df = df[[\"title\", \"label\"]]\n",
    "\n",
    "# 保存合并后的数据（可选）\n",
    "df.to_csv(\"/Users/jerry/Desktop/gossipcop_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jerry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 下载停用词（如果未下载）\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # 转为小写\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 去除标点\n",
    "    # 可选择性去除停用词\n",
    "    text = \" \".join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# 添加清洗后的文本（仅用于 TF-IDF 和 Word2Vec）\n",
    "df[\"clean_title\"] = df[\"title\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF + 逻辑回归 分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.47      0.60       533\n",
      "           1       0.85      0.97      0.91      1681\n",
      "\n",
      "    accuracy                           0.85      2214\n",
      "   macro avg       0.84      0.72      0.75      2214\n",
      "weighted avg       0.85      0.85      0.83      2214\n",
      "\n",
      "AUROC: 0.8711813860462314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# 使用清洗后的文本\n",
    "X = df[\"clean_title\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# 划分训练/验证/测试集（80/10/10）\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# TF-IDF 特征提取（可调 max_features 参数）\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# 训练逻辑回归模型\n",
    "clf_tfidf = LogisticRegression(max_iter=1000)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 预测及评估\n",
    "y_pred_tfidf = clf_tfidf.predict(X_test_tfidf)\n",
    "print(\"TF-IDF + 逻辑回归 分类报告：\")\n",
    "print(classification_report(y_test, y_pred_tfidf))\n",
    "print(\"AUROC:\", roc_auc_score(y_test, clf_tfidf.predict_proba(X_test_tfidf)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec + SVM 分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.23      0.35       533\n",
      "           1       0.80      0.98      0.88      1681\n",
      "\n",
      "    accuracy                           0.80      2214\n",
      "   macro avg       0.78      0.60      0.62      2214\n",
      "weighted avg       0.79      0.80      0.75      2214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 对训练集文本进行分词（基于清洗后的文本）\n",
    "sentences = [text.split() for text in X_train]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, min_count=1, workers=4)\n",
    "\n",
    "def avg_vector(text, model):\n",
    "    words = text.split()\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_train_w2v = np.array([avg_vector(text, w2v_model) for text in X_train])\n",
    "X_test_w2v = np.array([avg_vector(text, w2v_model) for text in X_test])\n",
    "\n",
    "# 使用 SVM 分类器\n",
    "svm_clf = SVC(probability=True)\n",
    "svm_clf.fit(X_train_w2v, y_train)\n",
    "y_pred_w2v = svm_clf.predict(X_test_w2v)\n",
    "\n",
    "print(\"Word2Vec + SVM 分类报告：\")\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类报告（标题长度特征）:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1065\n",
      "           1       0.76      1.00      0.86      3363\n",
      "\n",
      "    accuracy                           0.76      4428\n",
      "   macro avg       0.38      0.50      0.43      4428\n",
      "weighted avg       0.58      0.76      0.66      4428\n",
      "\n",
      "AUROC: 0.540773873092854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jerry/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jerry/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jerry/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# 加载数据\n",
    "df = pd.read_csv(\"/Users/jerry/Desktop/gossipcop_cleaned.csv\")\n",
    "\n",
    "X = df[\"title\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# 划分数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 特征提取：简单的长度统计特征\n",
    "def simple_length_features(texts):\n",
    "    features = []\n",
    "    for text in texts:\n",
    "        features.append([\n",
    "            len(text),                      # 标题字符总数\n",
    "            len(text.split()),              # 标题单词数\n",
    "            np.mean([len(w) for w in text.split()]),  # 单词平均长度\n",
    "        ])\n",
    "    return np.array(features)\n",
    "\n",
    "train_features = simple_length_features(X_train)\n",
    "test_features = simple_length_features(X_test)\n",
    "\n",
    "# 训练逻辑回归分类器\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(train_features, y_train)\n",
    "\n",
    "# 预测和评估\n",
    "y_pred = clf.predict(test_features)\n",
    "y_prob = clf.predict_proba(test_features)[:, 1]\n",
    "\n",
    "print(\"分类报告（标题长度特征）:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(\"AUROC:\", auc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
